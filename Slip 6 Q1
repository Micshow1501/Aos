import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
def remove_stop_words(input_file, output_file):
    stop_words = set(stopwords.words('english'))
    with open(input_file, 'r', encoding='utf-8') as file:
        passage = file.read()
    words = word_tokenize(passage)
    filtered_words = [word for word in words if word.lower() not in stop_words]
    modified_passage = ' '.join(filtered_words)
    with open(output_file, 'w', encoding='utf-8') as file:
        file.write(modified_passage)
if __name__ == "__main__":
    input_file = "input.txt"  # Replace with your input file
    output_file = "output.txt"  # Replace with your output file
    remove_stop_words(input_file, output_file)
    print("Stop words removed. Check the output file:", output_file)
